---
title: "wifi_fingerprinting"
author: "Stacey Jackson"
date: "13/05/2019"
output:
  word_document: default
  html_document: default
---

```{r}
if(require("pacman")=="FALSE"){
  install.packages('pacman')
  library('pacman')
  pacman::p_load(here, readxl, plyr, caret, dplyr, doParallel,
                 lubridate, corrplot, ggplot2, 
                 tidyverse, arules, arulesViz, rstudioapi,RMySQL,
                 plotly, padr, lubridate, forecast,zoo, htmltools, readr,h2o)
} else {
  library('pacman')
  pacman::p_load(here, readxl, plyr, caret, dplyr, doParallel,
                 lubridate, corrplot, ggplot2, 
                 tidyverse, arules, arulesViz, rstudioapi,RMySQL,
                 plotly, padr, lubridate, forecast,zoo,htmltools, readr,h2o)
}

devtools::install_github('cran/ggplot2')

h2o.init(nthreads=-1)

#Upload the files
trainingData<-read.csv(file="/Users/staceyjackson/Dropbox (Personal)/Ubiqum/May_2019_Wifi/Dataset/UJIndoorLoc/trainingData.csv", header=TRUE, sep=",")

validationData<-read.csv(file="/Users/staceyjackson/Dropbox (Personal)/Ubiqum/May_2019_Wifi/Dataset/UJIndoorLoc/validationData.csv", header=TRUE, sep=",")


```

####1. INVESTIGATE THE TRAINING SET
####1.1 Factors and timestamp
```{r}

#Look at the classes
#str(trainingData)
#sapply(trainingData, class)


#change 523-528 to factors - relative position, USERID, PHONEID, BUILDING ID
columns <- c(523:528)
trainingData[,columns] <- lapply(trainingData[,columns], as.factor)
validationData[,columns]<-lapply(validationData[,columns],as.factor)

#sapply(trainingData,class)


#change time to POSixt
trainingData$TIMESTAMP <- as.POSIXct(trainingData$TIMESTAMP,origin="1970-01-01")
validationData$TIMESTAMP <- as.POSIXct(validationData$TIMESTAMP,origin="1970-01-01")
```

#1.2. Initial plots of the trainingData
```{r}
plot(trainingData$BUILDINGID)
plot(trainingData$USERID)
plot(trainingData$SPACEID)

p <- ggplot(data = trainingData, mapping = aes(x =USERID, y=PHONEID))
p + geom_point()

p <- ggplot(data = validationData, mapping = aes(x =LONGITUDE, y=LATITUDE))
p + geom_point()

plot(trainingData$USERID)
plot(trainingData$PHONEID)

p <- ggplot(data=trainingData, mapping=aes(x=USERID, y=PHONEID))
p + geom_point()

p <- ggplot(data=trainingData, mapping=aes(x=USERID, y=BUILDINGID))
p + geom_point()

#plot of all three buildings' longitude & latitude showing each floor
P<-ggplot(trainingData, mapping=aes(x=LONGITUDE, y=LATITUDE))
P+geom_point()


#create separate training datasets for each building and plot longitude & latitude showing each floor
#trainingDataBuilding0<-trainingData%>%
 # filter(BUILDINGID==0)

#P<-ggplot(trainingDataBuilding0, mapping=aes(x=LONGITUDE, y=LATITUDE, colour=FLOOR))
#P+geom_point()

trainingDataBuilding1<-trainingData%>%
  filter(BUILDINGID==0)
P<-ggplot(trainingDataBuilding1, mapping=aes(x=LONGITUDE, y=LATITUDE, colour=FLOOR))
P+geom_point()

#trainingDataBuilding2<-trainingData%>%
#  filter(BUILDINGID==2)
#P<-ggplot(trainingDataBuilding2, mapping=aes(x=LONGITUDE, y=LATITUDE, colour=FLOOR))
#P+geom_point()

a <- htmltools::tagList()    
for(i in unique(trainingData$BUILDINGID)){
a[[i]] <- trainingData %>% dplyr:: filter(BUILDINGID == i) %>% plot_ly(type = "scatter3d",
       x = ~ LATITUDE,
       y = ~ LONGITUDE,
       z = ~ FLOOR,
       mode = 'markers')


}  
a[[1]]
a[[2]]
a[[3]]
```

#1.3. Delete WAPs with a mean of 100
```{r}

#TRAINING DATA
#means<-apply(trainingData[1:520], 2, mean)
#means<-as.data.frame(means)

mean<-0
for (i in 1:520){
  mean[i]<-mean(trainingData[,i])
}
mean[521:529]<-0
mean<-as.data.frame(mean)

#delete all the WAPs with a mean of =100
indices<-c()
indices[521:529]<-0
for (i in 1:520){
if (mean[i,]==100){
  indices[i]<- i
}
}
trainingData2<- trainingData[is.na(indices)]

trainingData2<-cbind(trainingData2,trainingData[,c(521:529)])

validationData2<-validationData[is.na(indices)]

validationData2<-cbind(validationData2,validationData[,c(521:529)])


```
#1.4 Phones with weak signals in training data
```{r}
phones <- split(trainingData2, trainingData2$PHONEID)

sapply(phones, function(x) {
         colMeans(x[, c(1:50)])
})

#variance of the phones
sapply(phones, var)
```
#1.5 WAPs with low variance in training data
```{r}

variance <- sapply(trainingData2[,c(1:465)], var)
not_low_variance_WAP <- sapply(variance, function(x){
 (x>5)
})
not_low_variance_WAP<- which(not_low_variance_WAP, arr.ind = TRUE)
not_low_variance_WAP<- as.data.frame(not_low_variance_WAP)

indices2<-not_low_variance_WAP[,1]
trainingData3<- trainingData2[(indices2)]

trainingData4<-cbind(trainingData3,trainingData2[,c(466:474)])

validationData3 <- validationData2[(indices2)]
validationData4<-cbind(validationData3,validationData[,c(521:529)])


```
#1.6 Change WAP values from 100 to -105 
```{r}

#TRAINING DATA
change_WAP_value <- apply(trainingData4[,c(1:430)], 2, function(x) {ifelse(x == 100, -105, x)})

change_WAP_value<-as.data.frame(change_WAP_value)

trainingData5<-cbind(change_WAP_value,trainingData2[,c(466:474)])

#VALIDATION DATA
change_WAP_value <- apply(validationData4[,c(1:430)], 2, function(x) {ifelse(x == 100, -105, x)})

change_WAP_value<-as.data.frame(change_WAP_value)

validationData5<-cbind(change_WAP_value,validationData4[,c(431:439)])

```

#1.7 Find WAP values >-30 
```{r}
#found WAPs with high values but what to do with them?


trainingData5$high_value <- apply(trainingData5[,c(1:430)], 1, max)

#by user
high_value_user <- select(trainingData5,USERID,high_value)%>%
  filter(high_value>-30)
high_value_user 

#by phone
high_value_phone <- select(trainingData5,PHONEID,high_value)%>%
  filter(high_value>-30)
high_value_phone 

#plots
high_value_phone<-as.data.frame(high_value_phone)
p <- ggplot(data=high_value_phone, mapping=aes(x=PHONEID, y=high_value))
p + geom_point()

high_value_user<-as.data.frame(high_value_user)
p <- ggplot(data=high_value_user, mapping=aes(x=USERID, y=high_value))
p + geom_point()

high_value_user6<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==6)
p <- ggplot(data=high_value_user6, mapping=aes(x=high_value))
p + geom_histogram()

high_value_user3<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==3)
p <- ggplot(data=high_value_user3, mapping=aes(x=high_value))
p + geom_histogram()

high_value_user6<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==6)
p <- ggplot(data=high_value_user6, mapping=aes(x=high_value))
p + geom_histogram(fill="steelblue") + labs(x = "WAP readings for User 6")

phone19<-trainingData5%>%
  select(PHONEID,high_value)%>%
  filter(PHONEID==19)
p <- ggplot(data=phone19, mapping=aes(x=high_value))
p + geom_histogram(fill="red") + labs(x = "WAP readings for Phone 19")

high_value_user10<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==10)
p <- ggplot(data=high_value_user10, mapping=aes(x=high_value))
p + geom_histogram()

high_value_user14<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==14)
p <- ggplot(data=high_value_user14, mapping=aes(x=high_value))
p + geom_histogram()

high_value_user15<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==15)
p <- ggplot(data=high_value_user15, mapping=aes(x=high_value))
p + geom_histogram()


high_value_user16<- trainingData5%>%
  select(USERID, high_value)%>%
  filter(USERID==16)
p <- ggplot(data=high_value_user16, mapping=aes(x=high_value))
p + geom_histogram()


```

#2. REMOVE VARIABLES - spaceID, relative position, userID, phoneID, timestamp, and user 6 from the training set.
```{r}

trainingData5 <- trainingData5 %>% filter(USERID != 6)

trainingData6<-trainingData5[,-c(435:440)]


validationData6<-validationData5[,-c(435:439)]
```

#3. CREATE SAMPLE OF THE TRAINING SET
```{r}

sample <- trainingData6 %>% group_by(FLOOR, BUILDINGID) %>% sample_n(10)

table(sample$FLOOR)
table(sample$BUILDINGID)


#a <- htmltools::tagList()    
#for(i in unique(sample$BUILDINGID)){
#a[[i]] <- sample %>% dplyr:: filter(BUILDINGID == i) %>% plot_ly(type = "scatter3d",
       #x = ~ LATITUDE,
       #y = ~ LONGITUDE,
       #z = ~ FLOOR,
       #mode = 'markers')


#}  
#a[[1]]
#a[[2]]
#a[[3]]
```


#4. PREDICT THE BUILDING
# 4.1 Random Forest
```{r}
# Load package
library(randomForest)

#Saving the waps in a vector - training
WAPs_training<-grep("WAP", names(trainingData6), value=T)

# Get the best mtry
bestmtry_rf<-tuneRF(trainingData6[WAPs_training], trainingData6$BUILDINGID, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

training.h2o <- as.h2o(trainingData6)
test.h2o <- as.h2o(validationData6)


#Train a random forest using that mtry 
y.dep <-434
x.indep <- c(1:430)
rf_h2o<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = training.h2o,ntree=100, mtries=10, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o)
varImp <- h2o.varimp(rf_h2o)
predicted_buildingRF<-as.data.frame(h2o.predict(rf_h2o, test.h2o))
postResample(predicted_buildingRF, validationData6$BUILDINGID)

confusionMatrix(predicted_buildingRF$predict, validationData6$BUILDINGID)

#GRADIENT BOOSTED MACHINE
gbm_h2o<- h2o.gbm(y=y.dep, x=x.indep, training_frame = training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o)
varImp <- h2o.varimp(gbm_h2o)
predicted_building_gbm<-as.data.frame(h2o.predict(gbm_h2o, test.h2o))
postResample(predicted_building_gbm, validationData6$BUILDINGID)

confusionMatrix(predicted_building_gbm$predict, validationData6$BUILDINGID)

```

#5. CHANGE COLUMN NAMES IN THE VALIDATION DATA & ADD IN BUILDING PREDICTIONS
```{r}

names(validationData6)[names(validationData6)=="BUILDINGID"] <- "original_building"

A<-predicted_buildingRF$predict

validationData6 <- cbind(validationData6, A)


names(validationData6)[names(validationData6)=="A"] <- "BUILDINGID"


```

#6. DIVIDE TRAINING & VALIDATION DATASETS INTO THREE - ONE FOR EACH BUILDING
```{r}
#Training
trainingDataBuilding0 <- trainingData6%>%
  filter(BUILDINGID==0 & FLOOR !=4)

trainingDataBuilding0$FLOOR <- as.character(trainingDataBuilding0$FLOOR)
trainingDataBuilding0$FLOOR <- as.factor(trainingDataBuilding0$FLOOR)

trainingDataBuilding0$BUILDINGID <- as.character(trainingDataBuilding0$BUILDINGID)
trainingDataBuilding0$BUILDINGID<- as.factor(trainingDataBuilding0$BUILDINGID)

#CHANGE FLOOR TO CHARACTER AND THEN BACK TO FACTOR AGAIN

trainingDataBuilding1 <- trainingData6%>%
  filter(BUILDINGID==1 & FLOOR !=4)

trainingDataBuilding1$FLOOR <- as.character(trainingDataBuilding1$FLOOR)
trainingDataBuilding1$FLOOR <- as.factor(trainingDataBuilding1$FLOOR)

trainingDataBuilding1$BUILDINGID <- as.character(trainingDataBuilding1$BUILDINGID)
trainingDataBuilding1$BUILDINGID<- as.factor(trainingDataBuilding1$BUILDINGID)

trainingDataBuilding2 <- trainingData6%>%
  filter(BUILDINGID==2)
trainingDataBuilding2$BUILDINGID <- as.character(trainingDataBuilding2$BUILDINGID)
trainingDataBuilding2$BUILDINGID<- as.factor(trainingDataBuilding2$BUILDINGID)

#Validation
validationDataBuilding0<-validationData6%>%
  filter(BUILDINGID==0 & FLOOR !=4)

validationDataBuilding0$FLOOR <- as.character(validationDataBuilding0$FLOOR)
validationDataBuilding0$FLOOR <- as.factor(validationDataBuilding0$FLOOR)


validationDataBuilding1<-validationData6%>%
  filter(BUILDINGID==1 & FLOOR !=4)

validationDataBuilding1$FLOOR <- as.character(validationDataBuilding1$FLOOR)
validationDataBuilding1$FLOOR <- as.factor(validationDataBuilding1$FLOOR)


validationDataBuilding2<-validationData6%>%
  filter(BUILDINGID==2)


```

#7 PREDICT FLOOR
#7.1 Building 0
```{r}
#Save WAPs in a vector
WAPs_training<-grep("WAP", names(trainingDataBuilding0), value=T)

# Get the best mtry
bestmtry_rf<-tuneRF(trainingDataBuilding0[WAPs_training], trainingDataBuilding0$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=40, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_floorB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_floorB0_rf, validationDataBuilding0$FLOOR)

confusionMatrix(predicted_floorB0_rf$predict, validationDataBuilding0$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_floorB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_floorB0_gbm, validationDataBuilding0$FLOOR)

confusionMatrix(predicted_floorB0_gbm$predict, validationDataBuilding0$FLOOR)


```

#7.2 Building 1
```{r}
#Save WAPs in a vector
WAPs_training<-grep("WAP", names(trainingDataBuilding1), value=T)

# Get the best mtry
bestmtry_rf<-tuneRF(trainingDataBuilding1[WAPs_training], trainingDataBuilding1$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry 
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=20, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_floorB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_floorB1_rf, validationDataBuilding1$FLOOR)

confusionMatrix(predicted_floorB1_rf$predict, validationDataBuilding1$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_floorB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_floorB1_gbm, validationDataBuilding1$FLOOR)

confusionMatrix(predicted_floorB1_gbm$predict, validationDataBuilding1$FLOOR)


```

#7.3 Building 2
```{r}

WAPs_training<-grep("WAP", names(trainingDataBuilding2), value=T)

# Get the best mtry
bestmtry_rf<-tuneRF(trainingDataBuilding2[WAPs_training], trainingDataBuilding2$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=40, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_floorB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_floorB2_rf, validationDataBuilding2$FLOOR)

confusionMatrix(predicted_floorB2_rf$predict, validationDataBuilding2$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_floorB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
postResample(predicted_floorB2_gbm, validationDataBuilding2$FLOOR)

confusionMatrix(predicted_floorB2_gbm$predict, validationDataBuilding2$FLOOR)



```

#8.CHANGE "FLOOR" IN  VALIDATION SETS FOR EACH BUILDING
```{r}

#BUILDING 0
names(validationDataBuilding0)[names(validationDataBuilding0)=="FLOOR"] <- "original_floor"
A<-predicted_floorB0_rf$predict
validationDataBuilding0 <- cbind(validationDataBuilding0, A)
names(validationDataBuilding0)[names(validationDataBuilding0)=="A"] <- "FLOOR"

#BUILDING1
names(validationDataBuilding1)[names(validationDataBuilding1)=="FLOOR"] <- "original_floor"
A<-predicted_floorB1_rf$predict
validationDataBuilding1 <- cbind(validationDataBuilding1, A)
names(validationDataBuilding1)[names(validationDataBuilding1)=="A"] <- "FLOOR"

#BUILDING2
names(validationDataBuilding2)[names(validationDataBuilding2)=="FLOOR"] <- "original_floor"
A<-predicted_floorB2_gbm$predict
validationDataBuilding2 <- cbind(validationDataBuilding2, A)
names(validationDataBuilding2)[names(validationDataBuilding2)=="A"] <- "FLOOR"

```

#9. PREDICT LONGITUDE
#9.1 Building 0
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding0), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_longB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_longB0_rf, validationDataBuilding0$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_longB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_longB0_gbm, validationDataBuilding0$LONGITUDE)

predictedValues <- predicted_longB0_rf
actualValues <- validationDataBuilding0$LONGITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError

#PLOTS OF THE ERRORS

plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding0, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

ggplot(validationDataBuilding0, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding0, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))
```
#9.2 Building 1
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding1), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding1[A], y=trainingDataBuilding1$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_longB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_longB1_rf, validationDataBuilding1$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_longB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_longB1_gbm, validationDataBuilding1$LONGITUDE)

predictedValues <- predicted_longB1_rf
actualValues <- validationDataBuilding1$LONGITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError
#PLOTS OF THE ERRORS

plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding1, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

ggplot(validationDataBuilding1, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding1, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))
```
#9.2 Building 2
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding2), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding2[A], y=trainingDataBuilding2$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_longB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_longB2_rf, validationDataBuilding2$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_longB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
postResample(predicted_longB2_gbm, validationDataBuilding2$LONGITUDE)

#PLOTS OF THE ERRORS

predictedValues <- predicted_longB2_rf
actualValues <- validationDataBuilding2$LONGITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError



plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding2, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_point() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LONGITUDE, y=absoluteError, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LONGITUDE, y=square_error, color=LONGITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Longitude", y= "Square Errors", title = "Test set: square errors")

ggplot(validationDataBuilding2, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding2, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))
```
#10.CHANGE "LONGITUDE" IN  VALIDATION SETS FOR EACH BUILDING
```{r}

#BUILDING 0
names(validationDataBuilding0)[names(validationDataBuilding0)=="LONGITUDE"] <- "original_long"
A<-predicted_longB0_rf$predict
validationDataBuilding0 <- cbind(validationDataBuilding0, A)
names(validationDataBuilding0)[names(validationDataBuilding0)=="A"] <- "LONGITUDE"

#BUILDING1
names(validationDataBuilding1)[names(validationDataBuilding1)=="LONGITUDE"] <- "original_long"
A<-predicted_longB1_rf$predict
validationDataBuilding1 <- cbind(validationDataBuilding1, A)
names(validationDataBuilding1)[names(validationDataBuilding1)=="A"] <- "LONGITUDE"

#BUILDING2
names(validationDataBuilding2)[names(validationDataBuilding2)=="LONGITUDE"] <- "original_long"
A<-predicted_longB2_rf$predict
validationDataBuilding2 <- cbind(validationDataBuilding2, A)
names(validationDataBuilding2)[names(validationDataBuilding2)=="A"] <- "LONGITUDE"

```
#11. PREDICT LATITUDE
#11.1 Building 0
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding0), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_latB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_latB0_rf, validationDataBuilding0$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_latB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_latB0_gbm, validationDataBuilding0$LATITUDE)

predictedValues <- predicted_latB0_gbm
actualValues <- validationDataBuilding0$LATITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError



plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding0, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding0, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_smooth(method = "loess") +
 # scale_x_log10() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

ggplot(validationDataBuilding0, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding0, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding0, mapping=aes(x=LONGITUDE, y=LATITUDE)) +
geom_point(mapping = aes(color = absoluteError))


```
#11.2 Building 1
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding1), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding1[A], y=trainingDataBuilding1$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_latB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_latB1_rf, validationDataBuilding1$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_latB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_latB1_gbm, validationDataBuilding1$LATITUDE)

predictedValues <- predicted_latB1_gbm
actualValues <- validationDataBuilding1$LATITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError



plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding1, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding1, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

ggplot(validationDataBuilding1, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding1, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding1, mapping=aes(x=LONGITUDE, y=LATITUDE)) +
geom_point(mapping = aes(color = absoluteError))
```
#11.3 Building 2
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding2), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding2[A], y=trainingDataBuilding2$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_latB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_latB2_rf, validationDataBuilding2$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_latB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
errors <- postResample(predicted_latB2_gbm, validationDataBuilding2$LATITUDE)

predictedValues <- predicted_latB2_gbm
actualValues <- validationDataBuilding2$LATITUDE

absoluteError <- predictedValues$predict - actualValues
square_error <- absoluteError * absoluteError



plot(actualValues, absoluteError)
plot(actualValues,square_error)

p<-ggplot(data=validationDataBuilding2, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_point() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LATITUDE, y=absoluteError, color=LATITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Latitude", y= "Absolute Errors", title = "Test set: absolute errors")

p<-ggplot(data=validationDataBuilding2, aes(x=LATITUDE, y=square_error, color=LATITUDE))
p+geom_smooth(method = "loess") +
  #scale_x_log10() +
  labs(x="Latitude", y= "Square Errors", title = "Test set: square errors")

p<-ggplot(data=validationDataBuilding2, aes(x=square_error))
p+geom_histogram()

ggplot(validationDataBuilding2, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding2, aes(x = square_error)) +
        geom_histogram(aes(y = ..count..))

ggplot(validationDataBuilding2, mapping=aes(x=LONGITUDE, y=LATITUDE)) +
geom_point(mapping = aes(color = absoluteError))

a <- htmltools::tagList()    
for(i in unique(validationDataBuilding2$LATITUDE)){
a[[i]] <- trainingData %>% dplyr:: filter(LATITUDE == i) %>% plot_ly(type = "scatter3d",
       x = ~ LATITUDE,
       y = ~ LONGITUDE,
       z = ~ absoluteError,
       mode = 'markers')


}  
a[[1]]
a[[2]]
a[[3]]
```
#12.FIND THE OUTLIERS IN BUILDING 2
```{r}
validationDataBuilding2$outliers<- validationDataBuilding2 %>%
  filter(LATITUDE == 4864800)

outliers <- validationDataBuilding2 %>% group_by(LATITUDE, LONGITUDE) %>% summarise(LATITUDE>4864800)

```

