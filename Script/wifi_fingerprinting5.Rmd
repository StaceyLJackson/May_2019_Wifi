---
title: "wifi_fingerprinting"
author: "Stacey Jackson"
date: "13/05/2019"
output:
  word_document: default
  html_document: default
---

```{r}
if(require("pacman")=="FALSE"){
  install.packages('pacman')
  library('pacman')
  pacman::p_load(here, readxl, plyr, caret, dplyr, doParallel,
                 lubridate, corrplot, ggplot2, 
                 tidyverse, arules, arulesViz, rstudioapi,RMySQL,
                 plotly, padr, lubridate, forecast,zoo, htmltools, readr,h2o)
} else {
  library('pacman')
  pacman::p_load(here, readxl, plyr, caret, dplyr, doParallel,
                 lubridate, corrplot, ggplot2, 
                 tidyverse, arules, arulesViz, rstudioapi,RMySQL,
                 plotly, padr, lubridate, forecast,zoo,htmltools, readr,h2o)
}

devtools::install_github('cran/ggplot2')

h2o.init(nthreads=-1)

#Upload the files
trainingData<-read.csv(file="/Users/staceyjackson/Dropbox (Personal)/Ubiqum/May_2019_Wifi/Dataset/UJIndoorLoc/trainingData.csv", header=TRUE, sep=",")

validationData<-read.csv(file="/Users/staceyjackson/Dropbox (Personal)/Ubiqum/May_2019_Wifi/Dataset/UJIndoorLoc/validationData.csv", header=TRUE, sep=",")


```

####1. INVESTIGATE THE TRAINING SET
####1.1 Factors and timestamp
```{r}

#Look at the classes
#str(trainingData)
#sapply(trainingData, class)


#change 523-528 to factors - relative position, USERID, PHONEID, BUILDING ID
columns <- c(523:528)
trainingData[,columns] <- lapply(trainingData[,columns], as.factor)
validationData[,columns]<-lapply(validationData[,columns],as.factor)

#sapply(trainingData,class)


#change time to POSixt
trainingData$TIMESTAMP <- as.POSIXct(trainingData$TIMESTAMP,origin="1970-01-01")
validationData$TIMESTAMP <- as.POSIXct(validationData$TIMESTAMP,origin="1970-01-01")
```

#1.2. Initial plots of the trainingData
```{r}

#plot of all three buildings' longitude & latitude showing each floor
P<-ggplot(trainingData, mapping=aes(x=LONGITUDE, y=LATITUDE))
P+geom_point()


a <- htmltools::tagList()    
for(i in unique(trainingData$BUILDINGID)){
a[[i]] <- trainingData %>% dplyr:: filter(BUILDINGID == i) %>% plot_ly(type = "scatter3d",
       x = ~ LATITUDE,
       y = ~ LONGITUDE,
       z = ~ FLOOR,
       mode = 'markers')


}  
a[[1]]
a[[2]]
a[[3]]
```

#1.3. Delete WAPs with a mean of 100
```{r}

#TRAINING DATA
#means<-apply(trainingData[1:520], 2, mean)
#means<-as.data.frame(means)

mean<-0
for (i in 1:520){
  mean[i]<-mean(trainingData[,i])
}
mean[521:529]<-0
mean<-as.data.frame(mean)

#delete all the WAPs with a mean of =100
indices<-c()
indices[521:529]<-0
for (i in 1:520){
if (mean[i,]==100){
  indices[i]<- i
}
}
trainingData2<- trainingData[is.na(indices)]

trainingData2<-cbind(trainingData2,trainingData[,c(521:529)])

validationData2<-validationData[is.na(indices)]

validationData2<-cbind(validationData2,validationData[,c(521:529)])


```
#1.4 Phones with weak signals in training data
```{r}

```
#1.5 WAPs with low variance in training data
```{r}

variance <- sapply(trainingData2[,c(1:465)], var)
not_low_variance_WAP <- sapply(variance, function(x){
 (x>5)
})
not_low_variance_WAP<- which(not_low_variance_WAP, arr.ind = TRUE)
not_low_variance_WAP<- as.data.frame(not_low_variance_WAP)

indices2<-not_low_variance_WAP[,1]
trainingData3<- trainingData2[(indices2)]

trainingData4<-cbind(trainingData3,trainingData2[,c(466:474)])

validationData3 <- validationData2[(indices2)]
validationData4<-cbind(validationData3,validationData[,c(521:529)])


```
#1.6 Change WAP values from 100 to -105 
```{r}

#TRAINING DATA
change_WAP_value <- apply(trainingData4[,c(1:430)], 2, function(x) {ifelse(x == 100, -105, x)})

change_WAP_value<-as.data.frame(change_WAP_value)

trainingData5<-cbind(change_WAP_value,trainingData2[,c(466:474)])

#VALIDATION DATA
change_WAP_value <- apply(validationData4[,c(1:430)], 2, function(x) {ifelse(x == 100, -105, x)})

change_WAP_value<-as.data.frame(change_WAP_value)

validationData5<-cbind(change_WAP_value,validationData4[,c(431:439)])

```

#1.7 Find WAP values >-30 
```{r}
#found WAPs with high values but what to do with them?




```

#2. REMOVE VARIABLES - spaceID, relative position, userID, phoneID, timestamp, and user 6 from the training set.
```{r}

trainingData5 <- trainingData5 %>% filter(USERID != 6)

trainingData6<-trainingData5[,-c(435:440)]


validationData6<-validationData5[,-c(435:439)]
```

#3. CREATE SAMPLE OF THE TRAINING SET
```{r}




#a <- htmltools::tagList()    
#for(i in unique(sample$BUILDINGID)){
#a[[i]] <- sample %>% dplyr:: filter(BUILDINGID == i) %>% plot_ly(type = "scatter3d",
       #x = ~ LATITUDE,
       #y = ~ LONGITUDE,
       #z = ~ FLOOR,
       #mode = 'markers')


#}  
#a[[1]]
#a[[2]]
#a[[3]]
```


#4. PREDICT THE BUILDING
# 4.1 Random Forest
```{r}
# Load package
library(randomForest)

#Saving the waps in a vector - training
WAPs_training<-grep("WAP", names(trainingData6), value=T)

# Get the best mtry
#bestmtry_rf<-tuneRF(trainingData6[WAPs_training], trainingData6$BUILDINGID, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

training.h2o <- as.h2o(trainingData6)
test.h2o <- as.h2o(validationData6)


#Train a random forest using that mtry 
y.dep <-434
x.indep <- c(1:430)
rf_h2o<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = training.h2o,ntree=100, mtries=10, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o)
varImp <- h2o.varimp(rf_h2o)
predicted_buildingRF<-as.data.frame(h2o.predict(rf_h2o, test.h2o))
postResample(predicted_buildingRF, validationData6$BUILDINGID)

confusionMatrix(predicted_buildingRF$predict, validationData6$BUILDINGID)

#GRADIENT BOOSTED MACHINE
gbm_h2o<- h2o.gbm(y=y.dep, x=x.indep, training_frame = training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o)
varImp <- h2o.varimp(gbm_h2o)
predicted_building_gbm<-as.data.frame(h2o.predict(gbm_h2o, test.h2o))
postResample(predicted_building_gbm, validationData6$BUILDINGID)

confusionMatrix(predicted_building_gbm$predict, validationData6$BUILDINGID)

```

#5. CHANGE COLUMN NAMES IN THE VALIDATION DATA & ADD IN BUILDING PREDICTIONS
```{r}

names(validationData6)[names(validationData6)=="BUILDINGID"] <- "original_building"

A<-predicted_buildingRF$predict

validationData6 <- cbind(validationData6, A)


names(validationData6)[names(validationData6)=="A"] <- "BUILDINGID"


```

#6. DIVIDE TRAINING & VALIDATION DATASETS INTO THREE - ONE FOR EACH BUILDING
```{r}
#Training
trainingDataBuilding0 <- trainingData6%>%
  filter(BUILDINGID==0 & FLOOR !=4)

trainingDataBuilding0$FLOOR <- as.character(trainingDataBuilding0$FLOOR)
trainingDataBuilding0$FLOOR <- as.factor(trainingDataBuilding0$FLOOR)

trainingDataBuilding0$BUILDINGID <- as.character(trainingDataBuilding0$BUILDINGID)
trainingDataBuilding0$BUILDINGID<- as.factor(trainingDataBuilding0$BUILDINGID)

#CHANGE FLOOR TO CHARACTER AND THEN BACK TO FACTOR AGAIN

trainingDataBuilding1 <- trainingData6%>%
  filter(BUILDINGID==1 & FLOOR !=4)

trainingDataBuilding1$FLOOR <- as.character(trainingDataBuilding1$FLOOR)
trainingDataBuilding1$FLOOR <- as.factor(trainingDataBuilding1$FLOOR)

trainingDataBuilding1$BUILDINGID <- as.character(trainingDataBuilding1$BUILDINGID)
trainingDataBuilding1$BUILDINGID<- as.factor(trainingDataBuilding1$BUILDINGID)

trainingDataBuilding2 <- trainingData6%>%
  filter(BUILDINGID==2)
trainingDataBuilding2$BUILDINGID <- as.character(trainingDataBuilding2$BUILDINGID)
trainingDataBuilding2$BUILDINGID<- as.factor(trainingDataBuilding2$BUILDINGID)

#Validation
validationDataBuilding0<-validationData6%>%
  filter(BUILDINGID==0 & FLOOR !=4)

validationDataBuilding0$FLOOR <- as.character(validationDataBuilding0$FLOOR)
validationDataBuilding0$FLOOR <- as.factor(validationDataBuilding0$FLOOR)


validationDataBuilding1<-validationData6%>%
  filter(BUILDINGID==1 & FLOOR !=4)

validationDataBuilding1$FLOOR <- as.character(validationDataBuilding1$FLOOR)
validationDataBuilding1$FLOOR <- as.factor(validationDataBuilding1$FLOOR)


validationDataBuilding2<-validationData6%>%
  filter(BUILDINGID==2)

```

#7 PREDICT FLOOR
#7.1 Building 0
```{r}
#Save WAPs in a vector
WAPs_training<-grep("WAP", names(trainingDataBuilding0), value=T)

# Get the best mtry
#bestmtry_rf<-tuneRF(trainingDataBuilding0[WAPs_training], trainingDataBuilding0$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=40, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_floorB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_floorB0_rf, validationDataBuilding0$FLOOR)

confusionMatrix(predicted_floorB0_rf$predict, validationDataBuilding0$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_floorB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_floorB0_gbm, validationDataBuilding0$FLOOR)

confusionMatrix(predicted_floorB0_gbm$predict, validationDataBuilding0$FLOOR)

#BUILDING0 
predictedValues <- predicted_floorB0_rf
actualValues <- validationDataBuilding0$FLOOR



validationDataBuilding0$absoluteError <- abs(predictedValues$predict - actualValues)






```

#7.2 Building 1
```{r}
#Save WAPs in a vector
WAPs_training<-grep("WAP", names(trainingDataBuilding1), value=T)

# Get the best mtry
#bestmtry_rf<-tuneRF(trainingDataBuilding1[WAPs_training], trainingDataBuilding1$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry 
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=20, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_floorB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_floorB1_rf, validationDataBuilding1$FLOOR)

confusionMatrix(predicted_floorB1_rf$predict, validationDataBuilding1$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_floorB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_floorB1_gbm, validationDataBuilding1$FLOOR)

confusionMatrix(predicted_floorB1_gbm$predict, validationDataBuilding1$FLOOR)


```

#7.3 Building 2
```{r}

WAPs_training<-grep("WAP", names(trainingDataBuilding2), value=T)

# Get the best mtry
#bestmtry_rf<-tuneRF(trainingDataBuilding2[WAPs_training], trainingDataBuilding2$FLOOR, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-433
x.indep <- c(1:430)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=40, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_floorB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_floorB2_rf, validationDataBuilding2$FLOOR)

confusionMatrix(predicted_floorB2_rf$predict, validationDataBuilding2$FLOOR)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_floorB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
postResample(predicted_floorB2_gbm, validationDataBuilding2$FLOOR)

confusionMatrix(predicted_floorB2_gbm$predict, validationDataBuilding2$FLOOR)



```

#8.CHANGE "FLOOR" IN  VALIDATION SETS FOR EACH BUILDING
```{r}

#BUILDING 0
names(validationDataBuilding0)[names(validationDataBuilding0)=="FLOOR"] <- "original_floor"
A<-predicted_floorB0_rf$predict
validationDataBuilding0 <- cbind(validationDataBuilding0, A)
names(validationDataBuilding0)[names(validationDataBuilding0)=="A"] <- "FLOOR"

#BUILDING1
names(validationDataBuilding1)[names(validationDataBuilding1)=="FLOOR"] <- "original_floor"
A<-predicted_floorB1_rf$predict
validationDataBuilding1 <- cbind(validationDataBuilding1, A)
names(validationDataBuilding1)[names(validationDataBuilding1)=="A"] <- "FLOOR"

#BUILDING2
names(validationDataBuilding2)[names(validationDataBuilding2)=="FLOOR"] <- "original_floor"
A<-predicted_floorB2_gbm$predict
validationDataBuilding2 <- cbind(validationDataBuilding2, A)
names(validationDataBuilding2)[names(validationDataBuilding2)=="A"] <- "FLOOR"
```
#FLOOR ERROR PLOT
```{r}



validationDataBuilding0$classification<-ifelse(predicted_floorB0_rf$predict== validationDataBuilding0$original_floor ,"correct", "wrong")

ggplot(validationDataBuilding0, aes(x = original_floor, y= predicted_floorB0_rf$predict, color = classification))+
  geom_jitter() +xlab("Original Floor") +ylab("Predicted Floor")

validationDataBuilding1$classification<-ifelse(predicted_floorB1_rf$predict== validationDataBuilding1$original_floor ,"correct", "wrong")

ggplot(validationDataBuilding1, aes(x = original_floor, y= predicted_floorB1_rf$predict, color = classification))+
  geom_jitter() +xlab("Original Floor") +ylab("Predicted Floor")

validationDataBuilding2$classification<-ifelse(predicted_floorB2_rf$predict== validationDataBuilding2$original_floor ,"correct", "wrong")

ggplot(validationDataBuilding2, aes(x = original_floor, y= predicted_floorB2_rf$predict, color = classification))+
  geom_jitter() +xlab("Original Floor") +ylab("Predicted Floor")


plot_ly(data=validationDataBuilding0, x = ~ original_floor, y = ~ predicted_floorB0_rf$predict, color = ~ classification)
```


#9. PREDICT LONGITUDE
#9.1 Building 0
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding0), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_longB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_longB0_rf, validationDataBuilding0$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_longB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_longB0_gbm, validationDataBuilding0$LONGITUDE)

predictedValues <- predicted_longB0_rf
actualValues <- validationDataBuilding0$LONGITUDE

absoluteError <- predictedValues$predict - actualValues


#PLOTS OF THE ERRORS



#ggplot(validationDataBuilding0, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))


```
#9.2 Building 1
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding1), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding1[A], y=trainingDataBuilding1$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_longB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_longB1_rf, validationDataBuilding1$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_longB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_longB1_gbm, validationDataBuilding1$LONGITUDE)

predictedValues <- predicted_longB1_rf
actualValues <- validationDataBuilding1$LONGITUDE

absoluteError <- predictedValues$predict - actualValues

#PLOTS OF THE ERRORS





#ggplot(validationDataBuilding1, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))


```
#9.2 Building 2
```{r}

A<-grep("WAP|FLOOR", names(trainingDataBuilding2), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding2[A], y=trainingDataBuilding2$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_longB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_longB2_rf, validationDataBuilding2$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_longB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
postResample(predicted_longB2_gbm, validationDataBuilding2$LONGITUDE)

#PLOTS OF THE ERRORS

predictedValues <- predicted_longB2_rf
actualValues <- validationDataBuilding2$LONGITUDE

absoluteError <- predictedValues$predict - actualValues


#ggplot(validationDataBuilding2, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))


```
#10.CHANGE "LONGITUDE" IN  VALIDATION SETS FOR EACH BUILDING
```{r}

#BUILDING 0
names(validationDataBuilding0)[names(validationDataBuilding0)=="LONGITUDE"] <- "original_long"
A<-predicted_longB0_rf$predict
validationDataBuilding0 <- cbind(validationDataBuilding0, A)
names(validationDataBuilding0)[names(validationDataBuilding0)=="A"] <- "LONGITUDE"

#BUILDING1
names(validationDataBuilding1)[names(validationDataBuilding1)=="LONGITUDE"] <- "original_long"
A<-predicted_longB1_rf$predict
validationDataBuilding1 <- cbind(validationDataBuilding1, A)
names(validationDataBuilding1)[names(validationDataBuilding1)=="A"] <- "LONGITUDE"

#BUILDING2
names(validationDataBuilding2)[names(validationDataBuilding2)=="LONGITUDE"] <- "original_long"
A<-predicted_longB2_rf$predict
validationDataBuilding2 <- cbind(validationDataBuilding2, A)
names(validationDataBuilding2)[names(validationDataBuilding2)=="A"] <- "LONGITUDE"

```
#11. PREDICT LATITUDE
#11.1 Building 0
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding0), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_latB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_latB0_rf, validationDataBuilding0$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_latB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_latB0_gbm, validationDataBuilding0$LATITUDE)

predictedValues <- predicted_latB0_rf
actualValues <- validationDataBuilding0$LATITUDE

absoluteError <- predictedValues$predict - actualValues


validationDataBuilding0$absoluteError <- abs(predictedValues$predict - actualValues)


ggplot(validationDataBuilding0, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

validationDataBuilding0$errorType<-ifelse(validationDataBuilding0$absoluteError<25,"low",
                                          ifelse(validationDataBuilding0$absoluteError>=25 & validationDataBuilding0$absoluteError<50, "medium", "high"))

#ggplot(validationDataBuilding0, mapping=aes(x=original_long, y=LATITUDE)) +
  geom_point(mapping = aes(color = errorType))


plot_ly(data=validationDataBuilding0, x = ~ original_long, y = ~ LATITUDE, color = ~ errorType)

plot_ly(data=validationDataBuilding0, x = ~ BUILDINGID, y = ~ original_floor, color = ~ errorType)


```
#11.2 Building 1
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding1), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding1[A], y=trainingDataBuilding1$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_latB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_latB1_rf, validationDataBuilding1$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_latB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_latB1_gbm, validationDataBuilding1$LATITUDE)

predictedValues <- predicted_latB1_gbm
actualValues <- validationDataBuilding1$LATITUDE

validationDataBuilding1$absoluteError <- abs(predictedValues$predict - actualValues)


ggplot(validationDataBuilding1, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

validationDataBuilding1$errorType<-ifelse(validationDataBuilding1$absoluteError<25,"low",
                                          ifelse(validationDataBuilding1$absoluteError>=25 & validationDataBuilding1$absoluteError<50, "medium", "high"))

ggplot(validationDataBuilding1, mapping=aes(x=original_long, y=LATITUDE)) +
  geom_point(mapping = aes(color = errorType))


plot_ly(data=validationDataBuilding1, x = ~ original_long, y = ~ LATITUDE, color = ~ errorType)

#outlier <- validationDataBuilding1%>%
#  filter(original_long <(-7550))
#outlier
```
#11.3 Building 2
```{r}

A<-grep("WAP|FLOOR|LONGITUDE", names(trainingDataBuilding2), value=T)

#bestmtry_rf<-tuneRF(x=trainingDataBuilding2[A], y=trainingDataBuilding2$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=144, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_latB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_latB2_rf, validationDataBuilding2$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_latB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
errors <- postResample(predicted_latB2_gbm, validationDataBuilding2$LATITUDE)


predictedValues <- predicted_latB2_gbm
actualValues <- validationDataBuilding2$LATITUDE

validationDataBuilding2$absoluteError <- abs(predictedValues$predict - actualValues)


ggplot(validationDataBuilding2, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

validationDataBuilding2$errorType<-ifelse(validationDataBuilding2$absoluteError<25,"low",
                                          ifelse(validationDataBuilding2$absoluteError>=25 & validationDataBuilding2$absoluteError<50, "medium", "high"))

ggplot(validationDataBuilding2, mapping=aes(x=original_long, y=LATITUDE)) +
  geom_point(mapping = aes(color = errorType))


plot_ly(data=validationDataBuilding2, x = ~ original_long, y = ~ LATITUDE, color = ~ errorType)

outlier <- validationDataBuilding2%>%
  filter(original_long <(-7550))
outlier

plot_ly(data=validationDataBuilding2, x = ~ BUILDINGID, y = ~ FLOOR, color = ~ errorType)



```
#12. PREDICT LONGITUDE with just the waps
#12.1 Building 0
```{r}
A<-grep("WAP", names(trainingDataBuilding0), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=185, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_longB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_longB0_rf, validationDataBuilding0$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_longB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_longB0_gbm, validationDataBuilding0$LONGITUDE)

predictedValues <- predicted_longB0_rf
actualValues <- validationDataBuilding0$LONGITUDE

absoluteError <- predictedValues$predict - actualValues


#PLOTS OF THE ERRORS

#ggplot(validationDataBuilding0, aes(x = absoluteError)) +
       # geom_histogram(aes(y = ..count..))

hist(absoluteError)
```
#12.2 Building 1
```{r}
A<-grep("WAP", names(trainingDataBuilding1), value=T)


bestmtry_rf<-tuneRF(x=trainingDataBuilding1[A], y=trainingDataBuilding1$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B1training.h2o <- as.h2o(trainingDataBuilding1)
B1test.h2o <- as.h2o(validationDataBuilding1)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B1<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B1training.h2o,ntree=100, mtries=72, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B1)
varImp <- h2o.varimp(rf_h2o_B1)
predicted_longB1_rf<-as.data.frame(h2o.predict(rf_h2o_B1, B1test.h2o))
postResample(predicted_longB1_rf, validationDataBuilding1$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B1 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B1training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B1)
varImp <- h2o.varimp(gbm_h2o_B1)
predicted_longB1_gbm<-as.data.frame(h2o.predict(gbm_h2o_B1, B1test.h2o))
postResample(predicted_longB1_gbm, validationDataBuilding1$LONGITUDE)

predictedValues <- predicted_longB1_rf
actualValues <- validationDataBuilding1$LONGITUDE

absoluteError <- predictedValues$predict - actualValues


#PLOTS OF THE ERRORS

#ggplot(validationDataBuilding0, aes(x = absoluteError)) +
       # geom_histogram(aes(y = ..count..))

hist(absoluteError)
```

#12.2 Building 2
```{r}
A<-grep("WAP", names(trainingDataBuilding2), value=T)


bestmtry_rf<-tuneRF(x=trainingDataBuilding2[A], y=trainingDataBuilding2$LONGITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B2training.h2o <- as.h2o(trainingDataBuilding2)
B2test.h2o <- as.h2o(validationDataBuilding2)


#Train a random forest using that mtry 
y.dep <-431
x.indep <- c(1:430,433)
rf_h2o_B2<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B2training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B2)
varImp <- h2o.varimp(rf_h2o_B2)
predicted_longB2_rf<-as.data.frame(h2o.predict(rf_h2o_B2, B2test.h2o))
postResample(predicted_longB2_rf, validationDataBuilding2$LONGITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B2training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B2)
varImp <- h2o.varimp(gbm_h2o_B2)
predicted_longB2_gbm<-as.data.frame(h2o.predict(gbm_h2o_B2, B2test.h2o))
postResample(predicted_longB2_gbm, validationDataBuilding2$LONGITUDE)

predictedValues <- predicted_longB2_rf
actualValues <- validationDataBuilding2$LONGITUDE

absoluteError <- predictedValues$predict - actualValues


#PLOTS OF THE ERRORS

#ggplot(validationDataBuilding0, aes(x = absoluteError)) +
       # geom_histogram(aes(y = ..count..))

hist(absoluteError)
```
#13. PREDICT LATITUDE without floor or longitude
#13.1 Building 0
```{r}

A<-grep("WAP", names(trainingDataBuilding0), value=T)

bestmtry_rf<-tuneRF(x=trainingDataBuilding0[A], y=trainingDataBuilding0$LATITUDE, ntreeTry=100,stepFactor=2,improve=0.05,trace=TRUE, plot=T) 

#RANDOM FOREST
B0training.h2o <- as.h2o(trainingDataBuilding0)
B0test.h2o <- as.h2o(validationDataBuilding0)


#Train a random forest using that mtry 
y.dep <-432
x.indep <- c(1:431,433)
rf_h2o_B0<-h2o.randomForest(y=y.dep,x=x.indep,training_frame = B0training.h2o,ntree=100, mtries=143, max_depth = 4,seed = 1122)
h2o.performance(rf_h2o_B0)
varImp <- h2o.varimp(rf_h2o_B0)
predicted_latB0_rf<-as.data.frame(h2o.predict(rf_h2o_B0, B0test.h2o))
postResample(predicted_latB0_rf, validationDataBuilding0$LATITUDE)

#GRADIENT BOOSTED MACHINE
gbm_h2o_B0 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = B0training.h2o, ntrees = 100, max_depth = 4, learn_rate = 0.01, seed = 1122)
h2o.performance(gbm_h2o_B0)
varImp <- h2o.varimp(gbm_h2o_B0)
predicted_latB0_gbm<-as.data.frame(h2o.predict(gbm_h2o_B0, B0test.h2o))
postResample(predicted_latB0_gbm, validationDataBuilding0$LATITUDE)

predictedValues <- predicted_latB0_rf
actualValues <- validationDataBuilding0$LATITUDE

absoluteError <- predictedValues$predict - actualValues


validationDataBuilding0$absoluteError <- abs(predictedValues$predict - actualValues)


ggplot(validationDataBuilding0, aes(x = absoluteError)) +
        geom_histogram(aes(y = ..count..))

validationDataBuilding0$errorType<-ifelse(validationDataBuilding0$absoluteError<25,"low",
                                          ifelse(validationDataBuilding0$absoluteError>=25 & validationDataBuilding0$absoluteError<50, "medium", "high"))

#ggplot(validationDataBuilding0, mapping=aes(x=original_long, y=LATITUDE)) +
  geom_point(mapping = aes(color = errorType))


plot_ly(data=validationDataBuilding0, x = ~ original_long, y = ~ LATITUDE, color = ~ errorType)

plot_ly(data=validationDataBuilding0, x = ~ BUILDINGID, y = ~ original_floor, color = ~ errorType)

hist(absoluteError)
```
#14. NORMALISE by row

```{r}

#To normalize for each row, you can use apply and then subtract the minimum from each column and divide by the difference between maximum and minimum
trainingData7 <- t(apply(trainingData6[,1:430], 1, function(x)(x-min(x))/(max(x)-min(x)))) 
```

